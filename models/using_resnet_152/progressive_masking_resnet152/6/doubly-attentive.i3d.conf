[train]
seed: 123 
model_type: AttentiveMNMTFeatures
patience: 10
max_epochs: 100
eval_freq: 0
eval_metrics: loss,bleu
# Tokenization was done with -a parameter of moses tokenizer
eval_filters:
eval_beam: 1
eval_batch_size: 32
save_best_metrics: True
eval_max_len: 100
n_checkpoints: 0
l2_reg: 1e-05
lr_decay: plateau
lr_decay_revert: False
lr_decay_factor: 0.5
lr_decay_patience: 2
gclip: 1
optimizer: adam
lr: 0.0004
batch_size: 128
save_path: ./ckpt
tensorboard_dir: ./tb_dir

[model]
direction: en:Text, feats:NumpySequence -> zh:Text
feat_name: feats
feat_dim: 2048 

att_type: mlp
att_bottleneck: hid
enc_dim: 512
dec_dim: 512
emb_dim: 1024
dropout_emb: 0.5
dropout_ctx: 0.5
dropout_out: 0.5
n_encoders: 2
tied_emb: 2way
bucket_by: en
max_len: 100

sampler_type: approximate
sched_sampling: 0
dec_init: zero
bos_type: emb

fusion_type: hierarchical
use_feat_pe: True
feat_max_len: 50


[data]
tok_root:  ../../../../data/bpe/tok_progressive_masking/6

train_set: {   'en': '${tok_root}/en/8000/train.min-0.en',
               'zh': '${tok_root}/train.6.zh',
            'feats': './resnet152.per_second_frame_train.id'}

val_set: {     'en': '${tok_root}/en/8000/val.min-0.en',
               'zh': '${tok_root}/val.6.zh',
            'feats': './resnet152.per_second_frame_val.id'}

public_test_set: {'en': '${tok_root}/en/8000/public_test.min-0.en',
               'feats': './resnet152.per_second_frame_public_test.id'}

[vocabulary]
en: ../../../../data/bpe/tok_progressive_masking/6/vocab/train.min-0.vocab.en
zh: ../../../../data/bpe/tok_progressive_masking/6/vocab/train.6.vocab.zh
