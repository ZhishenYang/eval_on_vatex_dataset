[train]
seed: 123 
model_type: Transformer
patience: 10
max_epochs: 100
eval_freq: 0
eval_metrics: loss,bleu
# Tokenization was done with -a parameter of moses tokenizer
eval_filters:
eval_beam: 1
eval_batch_size: 500
save_best_metrics: True
eval_max_len: 100
n_checkpoints: 0
l2_reg: 1e-05
lr_decay: False
lr_decay_revert: False
lr_decay_factor: 0.5
lr_decay_patience: 2
gclip: 1
optimizer: transformer
lr: 0.0442
batch_size: 1000
update_freq: 4
save_path: ./ckpt
tensorboard_dir: ./tb_dir

[model]
direction: en:Text -> zh:Text

sampler_type: token
bucket_by: en
max_len: 100


[data]
tok_root:  ../../../../data/bpe/tok_progressive_masking/10

train_set: {   'en': '${tok_root}/en/8000/train.min-0.en',
               'zh': '${tok_root}/train.10.zh'}

val_set: {     'en': '${tok_root}/en/8000/val.min-0.en',
               'zh': '${tok_root}/val.10.zh'}

public_test_set: {'en':'${tok_root}/en/8000/public_test.min-0.en'}

[vocabulary]

en: ../../../../data/bpe/tok_progressive_masking/10/vocab/train.min-0.vocab.en
zh: ../../../../data/bpe/tok_progressive_masking/10/vocab/train.10.vocab.zh
