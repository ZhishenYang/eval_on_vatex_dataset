[train]
seed: 123 
model_type: NMT
patience: 10
max_epochs: 100
eval_freq: 0
eval_metrics: loss,bleu
# Tokenization was done with -a parameter of moses tokenizer
eval_filters:
eval_beam: 1
eval_batch_size: 32
save_best_metrics: True
eval_max_len: 100
n_checkpoints: 0
l2_reg: 1e-05
lr_decay: plateau
lr_decay_revert: False
lr_decay_factor: 0.5
lr_decay_patience: 2
gclip: 1
optimizer: adam
lr: 0.0004
batch_size: 64
save_path: ./ckpt
tensorboard_dir: ./tb_dir

[model]
direction: en:Text -> zh:Text

att_type: mlp
att_bottleneck: hid
enc_dim: 512
dec_dim: 512
emb_dim: 1024
dropout_emb: 0.5
dropout_ctx: 0.5
dropout_out: 0.5
n_encoders: 2
tied_emb: 2way
bucket_by: en
max_len: 100

sampler_type: approximate
sched_sampling: 0
dec_init: zero
bos_type: emb


[data]
tok_root:  ../../../../data/bpe/tok_progressive_masking/10

train_set: {   'en': '${tok_root}/en/8000/train.min-0.en',
               'zh': '${tok_root}/train.10.zh'}

val_set: {     'en': '${tok_root}/en/8000/val.min-0.en',
               'zh': '${tok_root}/val.10.zh'}

public_test_set: {'en': '${tok_root}/en/8000/public_test.min-0.en'}

[vocabulary]
en: ../../../../data/bpe/tok_progressive_masking/10/vocab/train.min-0.vocab.en
zh: ../../../../data/bpe/tok_progressive_masking/10/vocab/train.10.vocab.zh
