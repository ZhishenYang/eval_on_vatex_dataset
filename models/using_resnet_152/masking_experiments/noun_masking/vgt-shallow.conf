[train]
seed: 123 
model_type: VGTShallow
patience: 10
max_epochs: 100
eval_freq: 0
eval_metrics: loss,bleu
# Tokenization was done with -a parameter of moses tokenizer
eval_filters:
eval_beam: 1
eval_batch_size: 500
save_best_metrics: True
eval_max_len: 100
n_checkpoints: 0
l2_reg: 1e-05
lr_decay: False
lr_decay_revert: False
lr_decay_factor: 0.5
lr_decay_patience: 2
gclip: 1
optimizer: transformer
lr: 0.0442
# batch_size*update_freq should be 4000.
# use largest batch_size as long as no OOM in GPU
batch_size: 1000
update_freq: 4
save_path: ./ckpt
tensorboard_dir: ./tb_dir

[model]
direction: en:Text, feats:NumpySequence -> zh:Text
feat_name: feats
feat_dim: 2048 

sampler_type: token
bucket_by: en
max_len: 100 

[data]
tok_root:  ../../../../data/bpe/tok_noun_masking

train_set: {   'en': '${tok_root}/en/8000/train.min-0.en',
               'zh': '${tok_root}/train.zh',
            'feats': './resnet152.per_second_frame_train.id'}

val_set: {     'en': '${tok_root}/en/8000/val.min-0.en',
               'zh': '${tok_root}/val.zh',
            'feats': './resnet152.per_second_frame_val.id'}

public_test_set: {'en': '${tok_root}/en/8000/public_test.min-0.en',
               'feats': './resnet152.per_second_frame_public_test.id'}

[vocabulary]
en: ../../../../data/bpe/tok_noun_masking/vocab/train.min-0.vocab.en
zh: ../../../../data/bpe/tok_noun_masking/vocab/train.vocab.zh
